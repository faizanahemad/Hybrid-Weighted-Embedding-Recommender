{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:18:42.732101Z",
     "start_time": "2019-11-17T19:18:38.801356Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import display, HTML\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:18:46.147130Z",
     "start_time": "2019-11-17T19:18:42.734167Z"
    }
   },
   "outputs": [],
   "source": [
    "from hwer.utils import normalize_affinity_scores_by_user_item, normalize_affinity_scores_by_user\n",
    "\n",
    "from hwer.utils import unit_length, build_user_item_dict, build_item_user_dict, cos_sim, shuffle_copy\n",
    "from hwer import HybridRecommender\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict, Tuple, Sequence, Type, Set, Optional\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from surprise.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm,tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T11:17:13.706779Z",
     "start_time": "2019-11-17T11:17:04.556428Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "users = pd.read_csv(\"users.csv\", sep=\"\\t\", engine=\"python\")\n",
    "movies = pd.read_csv(\"movies.csv\", sep=\"\\t\", engine=\"python\")\n",
    "ratings = pd.read_csv(\"ratings.csv\", sep=\"\\t\", engine=\"python\")\n",
    "\n",
    "users['user_id'] = users['user_id'].astype(str)\n",
    "movies['movie_id'] = movies['movie_id'].astype(str)\n",
    "ratings['movie_id'] = ratings['movie_id'].astype(str)\n",
    "ratings['user_id'] = ratings['user_id'].astype(str)\n",
    "\n",
    "print(users.shape, movies.shape, ratings.shape)\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import hwer\n",
    "reload(hwer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T11:17:14.283812Z",
     "start_time": "2019-11-17T11:17:13.710120Z"
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "movies.genres = movies.genres.fillna(\"[]\").apply(literal_eval)\n",
    "movies['year'] = movies['year'].fillna(-1).astype(int)\n",
    "\n",
    "movies.keywords = movies.keywords.fillna(\"[]\").apply(literal_eval)\n",
    "movies.keywords = movies.keywords.apply(lambda x: \" \".join(x))\n",
    "\n",
    "movies.tagline = movies.tagline.fillna(\"\")\n",
    "text_columns = [\"title\",\"keywords\",\"overview\",\"tagline\",\"original_title\"]\n",
    "movies[text_columns] = movies[text_columns].fillna(\"\")\n",
    "\n",
    "movies['text'] = movies[\"title\"] +\" \"+ movies[\"keywords\"] +\" \"+ movies[\"overview\"] +\" \"+ movies[\"tagline\"] +\" \"+ movies[\"original_title\"]\n",
    "movies[\"title_length\"] = movies[\"title\"].apply(len)\n",
    "movies[\"overview_length\"] = movies[\"overview\"].apply(len)\n",
    "movies[\"runtime\"] = movies[\"runtime\"].fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T11:17:18.255305Z",
     "start_time": "2019-11-17T11:17:14.286412Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings.head().values\n",
    "user_item_affinities = [[row[0], row[1], row[2]] for row in ratings.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T11:31:23.020669Z",
     "start_time": "2019-11-17T11:29:24.836084Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from hwer import MultiCategoricalEmbedding, FlairGlove100AndBytePairEmbedding, CategoricalEmbedding, NumericEmbedding, FlairGlove100Embedding\n",
    "from hwer import Feature, FeatureSet, ContentRecommendation, FeatureType\n",
    "\n",
    "embedding_mapper = {}\n",
    "embedding_mapper['gender'] = CategoricalEmbedding(n_dims=1)\n",
    "embedding_mapper['age'] = CategoricalEmbedding(n_dims=1)\n",
    "embedding_mapper['occupation'] = CategoricalEmbedding(n_dims=2)\n",
    "embedding_mapper['zip'] = CategoricalEmbedding(n_dims=2)\n",
    "\n",
    "embedding_mapper['text'] = FlairGlove100Embedding()\n",
    "embedding_mapper['numeric'] = NumericEmbedding(2)\n",
    "embedding_mapper['genres'] = MultiCategoricalEmbedding(n_dims=2)\n",
    "\n",
    "\n",
    "recsys = ContentRecommendation(embedding_mapper=embedding_mapper, knn_params=None, n_output_dims=8, rating_scale=(1,5))\n",
    "\n",
    "\n",
    "u1 = Feature(feature_name=\"gender\", feature_type=FeatureType.CATEGORICAL, values=users.gender.values)\n",
    "u2 = Feature(feature_name=\"age\", feature_type=FeatureType.CATEGORICAL, values=users.age.astype(str).values)\n",
    "u3 = Feature(feature_name=\"occupation\", feature_type=FeatureType.CATEGORICAL, values=users.occupation.astype(str).values)\n",
    "u4 = Feature(feature_name=\"zip\", feature_type=FeatureType.CATEGORICAL, values=users.zip.astype(str).values)\n",
    "user_data = FeatureSet([u1, u2, u3, u4])\n",
    "\n",
    "i1 = Feature(feature_name=\"text\", feature_type=FeatureType.STR, values=movies.text.values)\n",
    "i2 = Feature(feature_name=\"genres\", feature_type=FeatureType.MULTI_CATEGORICAL, values=movies.genres.values)\n",
    "i3 = Feature(feature_name=\"numeric\", feature_type=FeatureType.NUMERIC, values=movies[[\"title_length\", \"overview_length\", \"runtime\"]].values)\n",
    "item_data = FeatureSet([i1, i2, i3])\n",
    "\n",
    "kwargs = {}\n",
    "kwargs['user_data'] = user_data\n",
    "kwargs['item_data'] = item_data\n",
    "\n",
    "user_vectors, item_vectors = recsys.fit(users.user_id.values, movies.movie_id.values,\n",
    "               user_item_affinities, **kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T11:33:10.862180Z",
     "start_time": "2019-11-17T11:33:10.790275Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res, dist = zip(*recsys.find_items_for_user(user='1', positive=[], negative=[]))\n",
    "res = res[:100]\n",
    "\n",
    "preds = set(movies[movies.movie_id.isin(res)][\"title\"])\n",
    "actuals = set(movies.merge(ratings[ratings.user_id=='1'],on='movie_id')[\"title\"])\n",
    "\n",
    "len(preds.intersection(actuals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Base Hybrid Recsys\n",
    "# Do train-test split before testing\n",
    "# Use Content Vectors as initialisers\n",
    "# Train with fixed mu + bu for collaborative embeddings\n",
    "# Train prediction network with mu+bu+bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T08:12:15.306838Z",
     "start_time": "2019-11-15T08:11:50.626591Z"
    }
   },
   "outputs": [],
   "source": [
    "mean, bu, bi, spread, user_item_affinities = normalize_affinity_scores_by_user(user_item_affinities_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T13:24:49.183330Z",
     "start_time": "2019-11-15T13:24:49.173382Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FixedNorm(tf.keras.constraints.Constraint):\n",
    "    \"\"\"\n",
    "    Refer: \n",
    "    https://github.com/keras-team/keras/issues/1580\n",
    "    https://github.com/tensorflow/tensorflow/issues/33755\n",
    "    \"\"\"\n",
    "    def __init__(self, m=1.):\n",
    "        self.m = m\n",
    "\n",
    "    def __call__(self, p):\n",
    "        p = K.transpose(p)\n",
    "        unit_norm = p / (K.sqrt(K.sum(K.square(p), axis=0)) + 1e-6)\n",
    "        unit_norm = K.transpose(unit_norm)\n",
    "        return unit_norm * self.m\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'name': self.__class__.__name__, 'm': self.m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:43:16.152737Z",
     "start_time": "2019-11-15T17:43:16.148631Z"
    }
   },
   "outputs": [],
   "source": [
    "self = recsys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:11:54.826830Z",
     "start_time": "2019-11-15T17:11:54.814151Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T15:42:10.184103Z",
     "start_time": "2019-11-16T15:42:10.155862Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def __entity_entity_affinities_trainer__(entity_ids: List[str],\n",
    "                                         entity_entity_affinities: List[Tuple[str, str, float]],\n",
    "                                         entity_id_to_index: Dict[str, int],\n",
    "                                         vectors: np.ndarray,\n",
    "                                         n_output_dims: int,\n",
    "                                         lr=0.001,\n",
    "                                         epochs=15,\n",
    "                                         batch_size = 512) -> np.ndarray:\n",
    "    train_affinities, validation_affinities = train_test_split(entity_entity_affinities, test_size=0.5)\n",
    "    batch_size = batch_size\n",
    "\n",
    "    def generate_training_samples(affinities: List[Tuple[str, str, float]]):\n",
    "        def generator():\n",
    "            for i, j, r in affinities:\n",
    "                first_item = entity_id_to_index[i]\n",
    "                second_item = entity_id_to_index[j]\n",
    "                r = np.clip(r, -1.0, 1.0)\n",
    "                yield (first_item, second_item), r\n",
    "\n",
    "        return generator\n",
    "\n",
    "    output_shapes = (((), ()), ())\n",
    "    output_types = ((tf.int64, tf.int64), tf.float32)\n",
    "\n",
    "    train = tf.data.Dataset.from_generator(generate_training_samples(train_affinities),\n",
    "                                           output_types=output_types, output_shapes=output_shapes, )\n",
    "    validation = tf.data.Dataset.from_generator(generate_training_samples(validation_affinities),\n",
    "                                                output_types=output_types,\n",
    "                                                output_shapes=output_shapes, )\n",
    "    train = train.shuffle(batch_size).batch(batch_size)\n",
    "    validation = validation.shuffle(batch_size).batch(batch_size)\n",
    "\n",
    "    input_1 = keras.Input(shape=(1,))\n",
    "    input_2 = keras.Input(shape=(1,))\n",
    "\n",
    "    def build_base_network(embedding_size, vectors):\n",
    "        avg_value = np.mean(vectors)\n",
    "        i1 = keras.Input(shape=(1,))\n",
    "\n",
    "        embeddings_initializer = tf.keras.initializers.Constant(vectors)\n",
    "        embeddings = keras.layers.Embedding(len(entity_ids), embedding_size, input_length=1,\n",
    "                                            embeddings_initializer=embeddings_initializer)\n",
    "        # embeddings_constraint=FixedNorm()\n",
    "        # embeddings_constraint=tf.keras.constraints.unit_norm(axis=2)\n",
    "        item = embeddings(i1)\n",
    "        item = tf.keras.layers.Flatten()(item)\n",
    "        item = tf.keras.layers.GaussianNoise(0.01 * avg_value)(item)\n",
    "        dense = keras.layers.Dense(embedding_size * 8, activation=\"relu\", )\n",
    "        item = dense(item)\n",
    "        dense_0 = keras.layers.Dense(embedding_size * 8, activation=\"relu\", )\n",
    "        item = dense_0(item)\n",
    "        dense_1 = keras.layers.Dense(embedding_size * 8, activation=\"relu\", )\n",
    "        item = dense_1(item)\n",
    "        dense_2 = keras.layers.Dense(embedding_size, activation=\"linear\", )\n",
    "        item = dense_2(item)\n",
    "        item = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1))(item)\n",
    "        item = K.l2_normalize(item, axis=-1)\n",
    "        base_network = keras.Model(inputs=i1, outputs=item)\n",
    "        return base_network\n",
    "\n",
    "    bn = build_base_network(n_output_dims, vectors)\n",
    "\n",
    "    item_1 = bn(input_1)\n",
    "    item_2 = bn(input_2)\n",
    "\n",
    "    pred = tf.keras.layers.Dot(axes=1, normalize=True)([item_1, item_2])\n",
    "    #     pred = K.sum(item_1*item_2, keepdims=True, axis=-1)\n",
    "\n",
    "    #     pred = pred/2 + 0.5\n",
    "#     pred = K.clip(pred, -1, 1)\n",
    "    pred = K.tanh(pred)\n",
    "    print(pred, pred.shape)\n",
    "\n",
    "    model = keras.Model(inputs=[input_1, input_2],\n",
    "                        outputs=[pred])\n",
    "    #     encoder = tf.keras.Model(input_1, item_1)\n",
    "    encoder = bn\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss=['mean_squared_error'])\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0, patience=8, verbose=0, )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=2, min_lr=0.0001)\n",
    "    callbacks = [es, reduce_lr]\n",
    "\n",
    "    model.fit(train, epochs=epochs,\n",
    "              validation_data=validation, callbacks=callbacks)\n",
    "    \n",
    "    K.set_value(model.optimizer.lr, lr)\n",
    "\n",
    "    model.fit(validation, epochs=epochs,\n",
    "              validation_data=train, callbacks=callbacks)\n",
    "\n",
    "    return encoder.predict(\n",
    "        tf.data.Dataset.from_tensor_slices([entity_id_to_index[i] for i in entity_ids]).batch(batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T12:14:43.689013Z",
     "start_time": "2019-11-16T12:14:43.639160Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def __item_item_affinities_triplet_trainer__(self,\n",
    "                                   item_ids: List[str],\n",
    "                                   item_item_affinities: List[Tuple[str, str, int]],\n",
    "                                   item_vectors: np.ndarray) -> np.ndarray:\n",
    "    train_affinities, validation_affinities = train_test_split(item_item_affinities, test_size=0.5)\n",
    "    batch_size = 512\n",
    "\n",
    "    def generate_training_samples(affinities: List[Tuple[str, str, bool]], \n",
    "                                  random_pair_proba=0.25, random_pair_weight=0.25):\n",
    "        item_ids = list(self.item_id_to_index.keys())\n",
    "        item_close_dict = {}\n",
    "        item_far_dict = {}\n",
    "        for i,j,r in affinities:\n",
    "            assert r!=0\n",
    "            if r>0:\n",
    "                if i in item_close_dict:\n",
    "                    item_close_dict[i].append((j,r))\n",
    "                else:\n",
    "                    item_close_dict[i] = [(j,r)]\n",
    "                    \n",
    "                if j in item_close_dict:\n",
    "                    item_close_dict[j].append((i,r))\n",
    "                else:\n",
    "                    item_close_dict[j] = [(i,r)]\n",
    "            if r<0:\n",
    "                if i in item_far_dict:\n",
    "                    item_far_dict[i].append((j,r))\n",
    "                else:\n",
    "                    item_far_dict[i] = [(j,r)]\n",
    "                    \n",
    "                if j in item_far_dict:\n",
    "                    item_far_dict[j].append((i,r))\n",
    "                else:\n",
    "                    item_far_dict[j] = [(i,r)]\n",
    "        \n",
    "        total_items = len(item_ids)\n",
    "        def generator():\n",
    "            for i, j, r in affinities:\n",
    "                first_item = self.item_id_to_index[i]\n",
    "                second_item = self.item_id_to_index[j]\n",
    "                distant_item = None\n",
    "                if np.random.rand() < random_pair_proba:\n",
    "                    distant_item = item_ids[np.random.randint(0,len(item_ids))]\n",
    "                    distant_item = self.item_id_to_index[distant_item]\n",
    "                    distant_item_weight = random_pair_weight\n",
    "                    if r<0:\n",
    "                        distant_item, second_item = second_item, distant_item\n",
    "                        distant_item_weight, r = r, distant_item_weight\n",
    "                        \n",
    "                #\n",
    "                else:\n",
    "                    if r>0:\n",
    "                        if i in item_far_dict:\n",
    "                            distant_item,distant_item_weight = item_far_dict[i][np.random.randint(0,len(item_far_dict[i]))]\n",
    "                        else:\n",
    "                            distant_item = item_ids[np.random.randint(0,len(item_ids))]\n",
    "                            distant_item_weight = random_pair_weight\n",
    "                        distant_item = self.item_id_to_index[distant_item]\n",
    "                    else:\n",
    "                        if i in item_close_dict:\n",
    "                            distant_item,distant_item_weight = item_close_dict[i][np.random.randint(0,len(item_close_dict[i]))]\n",
    "                        else:\n",
    "                            distant_item = item_ids[np.random.randint(0,len(item_ids))]\n",
    "                            distant_item_weight = random_pair_weight\n",
    "                        distant_item = self.item_id_to_index[distant_item]\n",
    "                        distant_item, second_item = second_item, distant_item\n",
    "                        distant_item_weight, r = r, distant_item_weight\n",
    "#                 print((first_item, second_item, distant_item, r, distant_item_weight), r)      \n",
    "                yield (first_item, second_item, distant_item, r, distant_item_weight), r\n",
    "        return generator\n",
    "\n",
    "    output_shapes = (((), (), (), (), ()), ())\n",
    "    output_types = ((tf.int64, tf.int64, tf.int64, tf.float32, tf.float32), tf.float32)\n",
    "\n",
    "    train = tf.data.Dataset.from_generator(generate_training_samples(train_affinities),\n",
    "                                           output_types=output_types, output_shapes=output_shapes, )\n",
    "    validation = tf.data.Dataset.from_generator(generate_training_samples(validation_affinities),\n",
    "                                                output_types=output_types,\n",
    "                                                output_shapes=output_shapes,)\n",
    "    \n",
    "    train = train.shuffle(batch_size).batch(batch_size)\n",
    "    validation = validation.shuffle(batch_size).batch(batch_size)\n",
    "#     for t in iter(train):\n",
    "#         print(t)\n",
    "#     for t in iter(validation):\n",
    "#         print(t)\n",
    "#     return\n",
    "\n",
    "    input_1 = keras.Input(shape=(1,))\n",
    "    input_2 = keras.Input(shape=(1,))\n",
    "    input_3 = keras.Input(shape=(1,))\n",
    "    \n",
    "    close_weight = keras.Input(shape=(1,))\n",
    "    far_weight = keras.Input(shape=(1,))\n",
    "    \n",
    "    def build_base_network(embedding_size, item_vectors):\n",
    "        avg_value = np.mean(item_vectors)\n",
    "        i1 = keras.Input(shape=(1,))\n",
    "        \n",
    "    \n",
    "        embeddings_initializer = tf.keras.initializers.Constant(item_vectors)\n",
    "        embeddings = keras.layers.Embedding(len(item_ids), self.n_output_dims, input_length=1,\n",
    "                                     embeddings_initializer=embeddings_initializer)\n",
    "        item = embeddings(i1)\n",
    "        item = tf.keras.layers.Flatten()(item)\n",
    "        item = tf.keras.layers.GaussianNoise(0.001*avg_value)(item)\n",
    "        dense = keras.layers.Dense(embedding_size*8, activation=\"relu\",)\n",
    "        item = dense(item)\n",
    "        dense_0 = keras.layers.Dense(embedding_size*8, activation=\"relu\",)\n",
    "        item = dense_0(item)\n",
    "        dense_1 = keras.layers.Dense(embedding_size*8, activation=\"relu\",)\n",
    "        item = dense_1(item)\n",
    "        dense_2 = keras.layers.Dense(embedding_size, activation=\"linear\",)\n",
    "        item = dense_2(item)\n",
    "        #         item = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(item)\n",
    "        item = K.l2_normalize(item, axis=-1)\n",
    "        base_network = keras.Model(inputs=i1, outputs=item)\n",
    "        return base_network\n",
    "    \n",
    "    bn = build_base_network(self.n_output_dims, item_vectors)\n",
    "    \n",
    "        \n",
    "    \n",
    "    item_1 = bn(input_1)\n",
    "    item_2 = bn(input_2)\n",
    "    item_3 = bn(input_3)\n",
    "    \n",
    "    i1_i2_dist = tf.keras.layers.Dot(axes=1, normalize = True)([item_1, item_2])\n",
    "    i1_i2_dist = 1 - K.tanh(i1_i2_dist)\n",
    "    i1_i2_dist = close_weight * i1_i2_dist\n",
    "    \n",
    "    i1_i3_dist = tf.keras.layers.Dot(axes=1, normalize = True)([item_1, item_3])\n",
    "    i1_i3_dist = 1 - K.tanh(i1_i3_dist)\n",
    "    i1_i3_dist = i1_i3_dist / K.abs(far_weight)\n",
    "    #     pred = K.sum(item_1*item_2, keepdims=True, axis=-1)\n",
    "    \n",
    "    #     pred = pred/2 + 0.5\n",
    "    margin = 1.0\n",
    "    loss = K.relu(i1_i2_dist - i1_i3_dist + margin)\n",
    "    \n",
    "\n",
    "    model = keras.Model(inputs=[input_1, input_2, input_3, close_weight, far_weight ],\n",
    "                        outputs=[loss])\n",
    "    #     encoder = tf.keras.Model(input_1, item_1)\n",
    "    encoder = bn\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss=['mean_squared_error'])\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0, patience=5, verbose=0, )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.4, patience=2, min_lr=0.00001)\n",
    "    callbacks=[es, reduce_lr]\n",
    "\n",
    "\n",
    "    model.fit(train, epochs=30,\n",
    "              validation_data=validation, callbacks=callbacks)\n",
    "\n",
    "    model.fit(validation, epochs=30,\n",
    "              validation_data=train, callbacks=callbacks)\n",
    "\n",
    "    return encoder.predict(tf.data.Dataset.from_tensor_slices([self.item_id_to_index[i] for i in item_ids]).batch(batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T12:12:33.603188Z",
     "start_time": "2019-11-16T12:12:26.651832Z"
    }
   },
   "outputs": [],
   "source": [
    "class FakeRec:\n",
    "    def __init__(self, item_id_to_index, n_output_dims):\n",
    "        self.item_id_to_index = item_id_to_index\n",
    "        self.n_output_dims = n_output_dims\n",
    "        \n",
    "self = FakeRec(dict(zip(map(str,range(1,9)),range(0,8))),2)\n",
    "self\n",
    "item_ids = list(self.item_id_to_index.keys())\n",
    "ivs = np.random.randn(len(self.item_id_to_index.keys()),2)\n",
    "ivs\n",
    "\n",
    "random_item_item_aff = [(\"1\",\"2\",1),(\"2\",\"3\",1),(\"3\",\"4\",1),(\"1\",\"3\",1),(\"1\",\"4\",1),(\"2\",\"4\",1),\n",
    "                        (\"5\",\"6\",1),(\"6\",\"7\",1),(\"7\",\"8\",1),(\"5\",\"7\",1),(\"5\",\"8\",1),(\"6\",\"8\",1),\n",
    "                        (\"1\",\"5\",-1),(\"2\",\"6\",-1),(\"3\",\"7\",-1),(\"4\",\"8\",-1),\n",
    "                        (\"2\",\"5\",-1),(\"3\",\"6\",-1),(\"4\",\"7\",-1),(\"1\",\"8\",-1),\n",
    "                        (\"2\",\"6\",-1),(\"1\",\"7\",-1),(\"3\",\"8\",-1),(\"4\",\"5\",-1),\n",
    "                        (\"1\",\"6\",-1),(\"1\",\"8\",-1),(\"2\",\"7\",-1),(\"2\",\"8\",-1),\n",
    "                        (\"3\",\"8\",-1),(\"4\",\"5\",-1),(\"3\",\"5\",-1),(\"4\",\"6\",-1)]\n",
    "sample_embeddings = __item_item_affinities_trainer__(self,item_ids,random_item_item_aff,ivs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T15:42:36.046599Z",
     "start_time": "2019-11-16T15:42:15.172226Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "self = recsys\n",
    "item_ids = list(self.item_id_to_index.keys())\n",
    "random_item_item_aff = [(i,item_ids[np.random.randint(0,len(item_ids))],1 if int(i)%2==0 else -1) for i in item_ids]\n",
    "\n",
    "sample_embeddings = __entity_entity_affinities_trainer__(item_ids,random_item_item_aff,\n",
    "                                                         self.item_id_to_index,item_vectors, self.n_output_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:43:30.471481Z",
     "start_time": "2019-11-16T11:43:29.715193Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=ivs[:,0], y=ivs[:,1], hue=[int(i) for i in item_ids])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T11:43:35.267760Z",
     "start_time": "2019-11-16T11:43:34.203474Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=sample_embeddings[:,0], y=sample_embeddings[:,1], hue=[int(i) for i in item_ids])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:46:32.432164Z",
     "start_time": "2019-11-15T17:46:32.313642Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "actual_vs_pred = [(r,cos_sim(item_vectors[self.item_id_to_index[i]],item_vectors[self.item_id_to_index[j]])) for i,j,r in random_item_item_aff]\n",
    "np.sqrt(np.mean(np.square(np.array([a-p for a,p in actual_vs_pred]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:53:24.700636Z",
     "start_time": "2019-11-17T19:53:22.856299Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow.keras.backend as K\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "def generate_training_samples():\n",
    "    def generator():\n",
    "        for i in range(batch_size*10):\n",
    "            yield (np.random.rand(3),np.random.rand(3), np.random.rand()), 5\n",
    "    return generator\n",
    "\n",
    "output_shapes = (((3), (3), ()), ())\n",
    "output_types = (((tf.float32), (tf.float32), tf.float32), tf.float32)\n",
    "train = tf.data.Dataset.from_generator(generate_training_samples(),\n",
    "                                       output_types=output_types, output_shapes=output_shapes,)\n",
    "\n",
    "train = train.shuffle(batch_size).batch(batch_size)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "input_1 = keras.Input(shape=(3,))\n",
    "input_2 = keras.Input(shape=(3,))\n",
    "input_3 = keras.Input(shape=(1,))\n",
    "\n",
    "inputs = K.concatenate([input_1, input_2, input_3])\n",
    "inputs = tf.keras.layers.Flatten()(inputs)\n",
    "dense_1 = layers.Dense(16, activation='relu')\n",
    "\n",
    "\n",
    "x = dense_1(inputs)\n",
    "\n",
    "x = layers.Dense(8, activation=\"relu\")(x)\n",
    "\n",
    "pred = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "model = keras.Model(inputs=[input_1, input_2, input_3],\n",
    "                    outputs=[pred])\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)\n",
    "model.compile(optimizer=adam,\n",
    "              loss=['mean_squared_error'])\n",
    "\n",
    "\n",
    "model.fit(train, epochs=2)\n",
    "\n",
    "\n",
    "def generate_prediction_samples():\n",
    "    def generator():\n",
    "        for i in range(batch_size*2):\n",
    "            yield np.random.rand(3),np.random.rand(3), np.random.rand()\n",
    "    return generator\n",
    "\n",
    "output_shapes = ((3), (3), ())\n",
    "output_types = (tf.float32, tf.float32, tf.float32)\n",
    "predict = tf.data.Dataset.from_generator(generate_prediction_samples(),\n",
    "                                       output_types=output_types, output_shapes=output_shapes,)\n",
    "\n",
    "predict = predict.batch(batch_size)\n",
    "next(iter(predict))\n",
    "\n",
    "model.predict(next(iter(predict)))\n",
    "\n",
    "# model.predict_generator(iter(predict), steps=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:56:09.897178Z",
     "start_time": "2019-11-17T19:56:09.821635Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_prediction_samples():\n",
    "    def generator():\n",
    "        for i in range(batch_size*2):\n",
    "#             yield np.random.rand(3),np.random.rand(3)\n",
    "            yield [np.random.rand(3).reshape((-1,3)),np.random.rand(3).reshape((-1,3)), np.array([np.random.rand()])]\n",
    "\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "model.predict_generator(iter(generate_prediction_samples()()), steps=4)\n",
    "model.predict_generator(generate_prediction_samples()(), steps=4)\n",
    "\n",
    "\n",
    "# model.predict(next(iter(generate_prediction_samples()())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hybrid-recsys] *",
   "language": "python",
   "name": "conda-env-hybrid-recsys-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
